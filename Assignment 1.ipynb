{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30271f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original data: [9.2 6.1 5.7 4.1 2.2 7.3 8.6 9.9 8.6]\n",
      "bins: [2.2        4.76666667 7.33333333 9.9       ]\n",
      "disc_data: [3 2 2 1 0 2 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "#10(1)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# if bins=[v1,v2,v3,v4],\n",
    "# v1 < 1st interval(bin) <= v2\n",
    "# v2 < 2nd interval(bin) <= v3\n",
    "# v3 < 3rd interval(bin) <= v4\n",
    "# Equal width discretization function\n",
    "\n",
    "\n",
    "def equal_width(data, num_bins):\n",
    "    min = np.min(data)\n",
    "    max = np.max(data)\n",
    "    # hint: may use numpy's linspace\n",
    "    bins = np.linspace(min, max, num_bins + 1)\n",
    "    return bins\n",
    "\n",
    "\n",
    "# Continuous data\n",
    "data = np.array([9.2, 6.1, 5.7, 4.1, 2.2, 7.3, 8.6, 9.9, 8.6])\n",
    "\n",
    "# Define the number of bins\n",
    "num_bins = 3 # Suppose num_bins=3\n",
    "\n",
    "# compute intervals using equal width function\n",
    "bins = equal_width(data, num_bins)\n",
    "\n",
    "# Using bins, discretize the data by assigning it to bins\n",
    "disc_data = np.digitize(data, bins, right=True)\n",
    "\n",
    "# Display the results\n",
    "# show the results of original data, bins, disc_data, etc\n",
    "print(\"original data:\", data)\n",
    "print(\"bins:\", bins)\n",
    "print(\"disc_data:\", disc_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d3dd344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best split point in a numeric data array: 1.5\n"
     ]
    }
   ],
   "source": [
    "#10(2)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# (input) y: array of target values\n",
    "# (output) entropy: entropy value of y\n",
    "# for simplicity. assume binary class only\n",
    "# e.g., [0,0,1,0,0,1,1,1]\n",
    "\n",
    "def calculate_entropy(y):\n",
    "    class_counts = np.bincount(y)\n",
    "    probabilities = class_counts/len(y)\n",
    "    probabilities = probabilities[probabilities > 0]\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy\n",
    "\n",
    "# (input) y: array of target values\n",
    "# y_left: left interval\n",
    "# y_right: right interval\n",
    "# e.g., y=[1,2,3,4,5], y_left=[1,2], y_right=[3,4,5]\n",
    "# (output) info_gain: information gain between entropy(y) and average entropy after split.\n",
    "\n",
    "def information_gain(y, y_left, y_right):\n",
    "    entropy_before = calculate_entropy(y)\n",
    "    entropy_left = calculate_entropy(y_left)\n",
    "    entropy_right = calculate_entropy(y_right)\n",
    "    weighted_entropy_after = ((len(y_left) / len(y)) * entropy_left + (len(y_right) / len(y)) * entropy_right)\n",
    "    info_gain = entropy_before - weighted_entropy_after\n",
    "    return info_gain\n",
    "\n",
    "# compute entropy of y\n",
    "# calculate entropy of y_left and y_right, respectively\n",
    "# compute information gain\n",
    "\n",
    "def best_split(X, y):\n",
    "    best_info_gain = -1\n",
    "    best_split_point = None\n",
    "    \n",
    "    for value in np.unique(X):\n",
    "        y_left = y[X <= value]\n",
    "        y_right = y[X > value]\n",
    "        gain = information_gain(y, y_left, y_right)\n",
    "\n",
    "        if gain > best_info_gain:\n",
    "            best_info_gain = gain\n",
    "            best_split_point = value\n",
    "            \n",
    "    return best_split_point\n",
    "\n",
    "#initialize best information gain value=-1 & best split point=None\n",
    "# y_left : interval with y values <=value\n",
    "# y_right: interval with y values > value\n",
    "# compute information gain using two intervals y_left & y_right\n",
    "# update best_info & best_split_point\n",
    "\n",
    "X = np.array([1.5, 4.5, 2.0, 3.5, 1.5, 7.5, 3.5, 5.0])\n",
    "y = np.array([0, 0, 1, 0, 0, 1, 1, 1]) #binary\n",
    "\n",
    "# Best split point\n",
    "split_point = best_split(X, y)\n",
    "print(\"The best split point in a numeric data array:\", split_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f90f321f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent 1-itemsets:\n",
      " [['bread'], ['cock'], ['bournvita'], ['milk'], ['biscuit'], ['tea'], ['maggi'], ['coffee'], ['cornflakes'], ['sugar']]\n",
      "\n",
      "Frequent 2-itemsets:\n",
      " [['bournvita', 'bread'], ['bread', 'milk'], ['biscuit', 'bread'], ['bread', 'tea'], ['bread', 'maggi'], ['bread', 'coffee'], ['bread', 'sugar'], ['cock', 'coffee'], ['biscuit', 'cornflakes'], ['maggi', 'tea'], ['coffee', 'cornflakes'], ['coffee', 'sugar']]\n",
      "\n",
      "Frequent 3-itemsets:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "#11. Apriori algorithm\n",
    "\n",
    "import itertools\n",
    "\n",
    "trans_db=[ ['milk','bread','biscuit'], ['bread','milk','biscuit','cornflakes'],\n",
    "['bread','tea','bournvita'], ['jam','maggi','bread','milk'], ['maggi','tea','biscuit'],\n",
    "['bread','tea','bournvita'], ['maggi','tea','cornflakes'], ['maggi','bread','tea','biscuit'],\n",
    "['jam','maggi','bread','tea'], ['bread','milk'], ['coffee','cock','biscuit','cornflakes'],\n",
    "['coffee','cock','biscuit','cornflakes'], ['coffee','sugar','bournvita'], ['bread','coffee','cock'],\n",
    "['bread','sugar','biscuit'], ['coffee','sugar','cornflakes'], ['bread','sugar','bournvita'],\n",
    "['bread','coffee','sugar'], ['bread','coffee','sugar'], ['tea','milk','coffee','cornflakes'] ]\n",
    "\n",
    "min_support = 3\n",
    "\n",
    "# infreq_list : the set of infrequent itemsets (itemsets whose support is less than min_support)\n",
    "# 2D list structure. e.g., [ [‘bread’], [‘sugar’], [‘coffee’, bisciut’], ....]\n",
    "infreq_itemsets=[]\n",
    "# compute support value of an itemset\n",
    "def compute_support(itemset):\n",
    "    support=0\n",
    "    for trans in trans_db:\n",
    "        if set(itemset).intersection(set(trans)) == set(itemset):\n",
    "            support = support + 1\n",
    "    return support\n",
    "\n",
    "# Generate k+1 frequent itemsets from k frequent itemsets\n",
    "# (input) k_itemsets: k itemsets (itemset with length=k)\n",
    "# 2D list (length k). e.g. (k=2), [ [‘bread’, ‘sugar’], [‘coffee’, ‘bisciut’], ....]\n",
    "# (output) k_1_itemsets: (k+1) itemsets with length=k+1\n",
    "# 2D list (length k+1). e.g. (length=3), [ [‘bread’, ‘sugar’, ‘coffee’], [‘coffee’, ‘bisciut’, ‘jam’], ....]\n",
    "def generate_k_1_itemsets(k_itemsets):\n",
    "    k_1_itemsets = []\n",
    "    \n",
    "    k_itemsets = [itemset for itemset in k_itemsets if compute_support(itemset) >= min_support]\n",
    "    \n",
    "    for i in range(len(k_itemsets)):\n",
    "        for j in range(i + 1, len(k_itemsets)):\n",
    "            new_itemset = sorted(list(set(k_itemsets[i]).union(set(k_itemsets[j]))))\n",
    "            \n",
    "            if len(new_itemset) == len(k_itemsets[0]) + 1:\n",
    "                if compute_support(new_itemset) >= min_support:\n",
    "                    if new_itemset not in k_1_itemsets:\n",
    "                        k_1_itemsets.append(new_itemset)\n",
    "                else:\n",
    "                    if new_itemset not in infreq_itemsets:\n",
    "                        infreq_itemsets.append(new_itemset)\n",
    "    \n",
    "    return k_1_itemsets\n",
    "\n",
    "# In k_itemsets, remove itemsets whose support < min_support\n",
    "# k_i_itemsets: frequent (k_+1) itemsets (support >= min_support)\n",
    "# create ‘new_itemset’ by combining i and J\n",
    "# skip ‘new_itemset’ if\n",
    "# 1) i == j or\n",
    "# 2) ‘new_itemset’ is in in ‘infreq_itemsets’ or\n",
    "# 3) length is not (k+1) or\n",
    "# 4) support of ‘new_itemset’ < min_support\n",
    "# original 1-items (wihout considering min_support values)\n",
    "\n",
    "\n",
    "all_list_items = list(set(item for sublist in trans_db for item in sublist))\n",
    "\n",
    "# Frequent 1-itemsets, 2-itemsets, 3-itemsets\n",
    "item_1 = [[item] for item in all_list_items if compute_support([item]) >= min_support]\n",
    "print(\"Frequent 1-itemsets:\\n\", item_1)\n",
    "\n",
    "item_2 = generate_k_1_itemsets(item_1)\n",
    "print(\"\\nFrequent 2-itemsets:\\n\", item_2)\n",
    "\n",
    "item_3 = generate_k_1_itemsets(item_2)\n",
    "print(\"\\nFrequent 3-itemsets:\\n\", item_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ba673c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Equal Width Discretization:\n",
      "Feature sepal length (cm): [4.3  5.02 5.74 6.46 7.18 7.9 ]\n",
      "Feature sepal width (cm): [2.   2.48 2.96 3.44 3.92 4.4 ]\n",
      "Feature petal length (cm): [1.   2.18 3.36 4.54 5.72 6.9 ]\n",
      "Feature petal width (cm): [0.1  0.58 1.06 1.54 2.02 2.5 ]\n",
      "\n",
      "Equal Frequency Discretization:\n",
      "Feature sepal length (cm): [4.3  5.   5.6  6.1  6.52 7.9 ]\n",
      "Feature sepal width (cm): [2.  2.7 3.  3.1 3.4 4.4]\n",
      "Feature petal length (cm): [1.   1.5  3.9  4.64 5.32 6.9 ]\n",
      "Feature petal width (cm): [0.1  0.2  1.16 1.5  1.9  2.5 ]\n"
     ]
    }
   ],
   "source": [
    "#12\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "data = load_iris()\n",
    "features = data.data\n",
    "target_names = data.feature_names\n",
    "\n",
    "for method, method_label in zip(['uniform', 'quantile'], ['Equal Width', 'Equal Frequency']):\n",
    "    print(f'\\n{method_label} Discretization:')\n",
    "    \n",
    "    discretizer = KBinsDiscretizer(n_bins = 5, encode = 'ordinal', strategy = method, subsample = None)\n",
    "    discretizer.fit(features)\n",
    "    for feature_name, bins in zip(target_names, discretizer.bin_edges_):\n",
    "        print(f\"Feature {feature_name}: {bins}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e67912c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "      support                                           itemsets\n",
      "0        0.2                                            (Apple)\n",
      "1        0.4                                             (Corn)\n",
      "2        0.2                                             (Dill)\n",
      "3        0.8                                             (Eggs)\n",
      "4        0.2                                        (Ice cream)\n",
      "..       ...                                                ...\n",
      "144      0.4        (Onion, Nutmeg, Kidney Beans, Eggs, Yogurt)\n",
      "145      0.2                (Onion, Nutmeg, Eggs, Milk, Yogurt)\n",
      "146      0.2        (Onion, Nutmeg, Kidney Beans, Milk, Yogurt)\n",
      "147      0.2  (Onion, Dill, Nutmeg, Kidney Beans, Eggs, Yogurt)\n",
      "148      0.2  (Onion, Nutmeg, Kidney Beans, Eggs, Milk, Yogurt)\n",
      "\n",
      "[149 rows x 2 columns]\n",
      "\n",
      "Association Rules:\n",
      "            antecedents                                consequents  \\\n",
      "0              (Apple)                                     (Eggs)   \n",
      "1              (Apple)                             (Kidney Beans)   \n",
      "2              (Apple)                                     (Milk)   \n",
      "3               (Corn)                                     (Eggs)   \n",
      "4               (Corn)                                (Ice cream)   \n",
      "...                ...                                        ...   \n",
      "1028  (Yogurt, Nutmeg)          (Kidney Beans, Onion, Milk, Eggs)   \n",
      "1029      (Milk, Eggs)      (Kidney Beans, Onion, Nutmeg, Yogurt)   \n",
      "1030    (Yogurt, Eggs)        (Kidney Beans, Onion, Milk, Nutmeg)   \n",
      "1031    (Milk, Yogurt)        (Kidney Beans, Onion, Nutmeg, Eggs)   \n",
      "1032          (Nutmeg)  (Onion, Kidney Beans, Eggs, Milk, Yogurt)   \n",
      "\n",
      "      antecedent support  consequent support  support  confidence      lift  \\\n",
      "0                    0.2                 0.8      0.2         1.0  1.250000   \n",
      "1                    0.2                 1.0      0.2         1.0  1.000000   \n",
      "2                    0.2                 0.6      0.2         1.0  1.666667   \n",
      "3                    0.4                 0.8      0.2         0.5  0.625000   \n",
      "4                    0.4                 0.2      0.2         0.5  2.500000   \n",
      "...                  ...                 ...      ...         ...       ...   \n",
      "1028                 0.4                 0.2      0.2         0.5  2.500000   \n",
      "1029                 0.4                 0.4      0.2         0.5  1.250000   \n",
      "1030                 0.4                 0.2      0.2         0.5  2.500000   \n",
      "1031                 0.4                 0.4      0.2         0.5  1.250000   \n",
      "1032                 0.4                 0.2      0.2         0.5  2.500000   \n",
      "\n",
      "      leverage  conviction  zhangs_metric  \n",
      "0         0.04         inf       0.250000  \n",
      "1         0.00         inf       0.000000  \n",
      "2         0.08         inf       0.500000  \n",
      "3        -0.12         0.4      -0.500000  \n",
      "4         0.12         1.6       1.000000  \n",
      "...        ...         ...            ...  \n",
      "1028      0.12         1.6       1.000000  \n",
      "1029      0.04         1.2       0.333333  \n",
      "1030      0.12         1.6       1.000000  \n",
      "1031      0.04         1.2       0.333333  \n",
      "1032      0.12         1.6       1.000000  \n",
      "\n",
      "[1033 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "#13\n",
    "\n",
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "transactions = [\n",
    "    ['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "    ['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "    ['Milk', 'Apple', 'Kidney Beans', 'Eggs'],\n",
    "    ['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt'],\n",
    "    ['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs']\n",
    "]\n",
    "\n",
    "df = pd.get_dummies(pd.DataFrame(transactions).stack()).groupby(level = 0).sum()\n",
    "df = df.astype(bool)\n",
    "\n",
    "frequent_item = apriori(df, min_support = 0.1, use_colnames = True)\n",
    "rules_value = association_rules(frequent_item, metric='confidence', min_threshold = 0.5)\n",
    "\n",
    "print(\"Frequent Itemsets:\\n\", frequent_item)\n",
    "print(\"\\nAssociation Rules:\\n\", rules_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc5df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
